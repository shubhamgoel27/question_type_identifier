{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "import codecs\n",
    "import keras\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Embedding, Dropout, LSTM, Bidirectional\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the data\n",
    "\n",
    "Let's read in the data into a **Pandas dataframe**. We have the exact string text and the corresponding label associated with it. The objective is to create a model which is able to classify a new piece of text in the correct category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"LabelledData (1).txt\", delimiter=\",,, \", header=None, names=[\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how did serfdom develop in and then leave russ...</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what films featured the character popeye doyle ?</td>\n",
       "      <td>what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how can i find a list of celebrities ' real na...</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what fowl grabs the spotlight after the chines...</td>\n",
       "      <td>what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is the full form of .com ?</td>\n",
       "      <td>what</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label\n",
       "0  how did serfdom develop in and then leave russ...  unknown\n",
       "1  what films featured the character popeye doyle ?      what\n",
       "2  how can i find a list of celebrities ' real na...  unknown\n",
       "3  what fowl grabs the spotlight after the chines...     what\n",
       "4                   what is the full form of .com ?      what"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "what           607\n",
       "who            401\n",
       "unknown        272\n",
       "affirmation    104\n",
       "when            96\n",
       " what            2\n",
       " who             1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Basic data cleaning\n",
    "\n",
    "There are some categories which are equivalent but are stored differently due to an extra space. Let's correct that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label[df.label ==\" what\"] = \"what\"\n",
    "df.label[df.label ==\" who\"] = \"who\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "what           609\n",
       "who            402\n",
       "unknown        272\n",
       "affirmation    104\n",
       "when            96\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_to_number_map = {\"what\":0, \"who\":1, \"unknown\":2, \"affirmation\":3, \"when\":4}\n",
    "number_to_question_map = {v: k for k, v in question_to_number_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label = df.label.map(question_to_number_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>has anyone used this on a brick driveway ?</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>what do flatfish eat ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>what did shostakovich write for rostropovich ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>where does your hair grow the fastest ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>what type of bridge is the golden gate bridge ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "1421       has anyone used this on a brick driveway ?       3\n",
       "636                            what do flatfish eat ?       0\n",
       "617    what did shostakovich write for rostropovich ?       0\n",
       "461           where does your hair grow the fastest ?       2\n",
       "485   what type of bridge is the golden gate bridge ?       0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df.text.values, df.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is ready to be processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['has anyone used this on a brick driveway ? ',\n",
       "       'what do flatfish eat ? ',\n",
       "       'what did shostakovich write for rostropovich ? ',\n",
       "       'where does your hair grow the fastest ? ',\n",
       "       'what type of bridge is the golden gate bridge ? '], dtype=object)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 GloVe vectors\n",
    "\n",
    "We'll use pre-trained Global Vectors (**GloVe**) model to convert the individual words of our input questions into representational vectors for which every dimension (50 such dimensions here) represents an abstract language feature regarding the word.\n",
    "\n",
    "These embeddings are freely available online at https://www.kaggle.com/devjyotichandra/glove6b50dtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_FILE_PATH  = \"glove.6B.50d.txt\"\n",
    "embeddings_index = {}\n",
    "f = codecs.open(GLOVE_FILE_PATH,'r','utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 4000\n",
    "maxlen = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tokenizer\n",
    "\n",
    "The tokenizer converts the sentences into individual words, also taking care of some basic pre-processing steps such as converting words into lowercase, etc. \n",
    "\n",
    "The words are also converted into numbers, based on a fixed mapping which the tokenizer stores as a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3675 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(x)\n",
    "sequences = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1483, 20)\n",
      "Shape of label tensor: (1483, 5)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "labels = to_categorical(np.asarray(y))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer is also pickled so that it doesn't need to be created again later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Creating Train and validation data for training\n",
    "\n",
    "Since we have a very small dataset, we try and use as much as possible for training (90%) and the remaining 10% for validation. Once satisfied, we'll randomly test on some questions from http://cogcomp.org/Data/QA/QC/train_1000.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing and validation set number for different types of questions\n",
      "['what', 'who', 'unknown', 'affirmation', 'when']\n",
      "[553. 357. 253.  95.  76.]\n",
      "[56. 45. 19.  9. 20.]\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.9\n",
    "nb_validation_samples = int(train_split * data.shape[0])\n",
    "x_train = data[:nb_validation_samples]\n",
    "y_train = labels[:nb_validation_samples]\n",
    "x_val = data[nb_validation_samples:]\n",
    "y_val = labels[nb_validation_samples:]\n",
    "\n",
    "print('Traing and validation set number for different types of questions')\n",
    "print(list(question_to_number_map.keys()))\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approx ratio of all classes is consistent in the train and val set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initializing the Embedding layer\n",
    "We initialize the embedding layer with the GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding Layer Initialized!\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=True)\n",
    "\n",
    "print('Word Embedding Layer Initialized!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Defining the network architecture\n",
    "\n",
    "We're using a Bi-directional LSTM architecture. The input first passes through the Embedding layer which converts them into meaningful language representations. The Embedding layer is followed by a Bidirectional LSTM layer, which is followed by a Dropout, Dense, Dropout layer sequentially. The final Dense layer outputs 5 values, corresponding to the 5 types of questions we want to differentiate between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "model fitting - Bidirectional LSTM\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 20, 50)            183800    \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 200)               120800    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 325,205\n",
      "Trainable params: 325,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "sequence_input = Input(shape=(maxlen,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "dropout = Dropout(0.5)(l_lstm)\n",
    "dense2 = Dense(100)(dropout)\n",
    "dropout2 = Dropout(0.5)(dense2)\n",
    "preds = Dense(5, activation='softmax')(dropout2)\n",
    "model = Model(sequence_input, preds)\n",
    "print(\"model fitting - Bidirectional LSTM\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training the model\n",
    "\n",
    "The loss function and the metric to measure results on is defined. The **Adam** optimizer is used, with learning rate set to the default value of 0.001 for updating the gradients. The model is trained for 10 epochs with a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epoch_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 1334 samples, validate on 149 samples\n",
      "Epoch 1/10\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.9227 - acc: 0.6799 - val_loss: 0.4648 - val_acc: 0.8456\n",
      "Epoch 2/10\n",
      "1334/1334 [==============================] - 1s 540us/step - loss: 0.2389 - acc: 0.9408 - val_loss: 0.3031 - val_acc: 0.9329\n",
      "Epoch 3/10\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.1028 - acc: 0.9753 - val_loss: 0.2650 - val_acc: 0.9463\n",
      "Epoch 4/10\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0721 - acc: 0.9843 - val_loss: 0.2900 - val_acc: 0.9664\n",
      "Epoch 5/10\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 0.0439 - acc: 0.9910 - val_loss: 0.3768 - val_acc: 0.9262\n",
      "Epoch 6/10\n",
      "1334/1334 [==============================] - 1s 532us/step - loss: 0.0266 - acc: 0.9925 - val_loss: 0.3448 - val_acc: 0.9530\n",
      "Epoch 7/10\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0145 - acc: 0.9978 - val_loss: 0.3754 - val_acc: 0.9463\n",
      "Epoch 8/10\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0212 - acc: 0.9948 - val_loss: 0.3727 - val_acc: 0.9597\n",
      "Epoch 9/10\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0184 - acc: 0.9970 - val_loss: 0.4225 - val_acc: 0.9262\n",
      "Epoch 10/10\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0129 - acc: 0.9963 - val_loss: 0.3990 - val_acc: 0.9597\n",
      "149/149 [==============================] - 0s 181us/step\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=epoch_num, batch_size=batch_size)\n",
    "score, acc = model.evaluate(x_val, y_val,\n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieves a validation accuracy of **95.97%**, which is pretty good. The model weights along with the architecture are saved in a file so that it can directly be used later, without requiring re-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"identity_predict.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using the trained model\n",
    "The saved file contains  \n",
    "    - the model architecture\n",
    "    - The model weights\n",
    "and can be loaded easily to use for the purpose of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"identity_predict.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the tokenizer which was pickled during the processing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(question):\n",
    "    x=[]\n",
    "    x.append(question)\n",
    "    x_seq = pad_sequences(tokenizer.texts_to_sequences(x), maxlen=maxlen, padding='post')\n",
    "    pred = np.argmax(model.predict(x_seq))\n",
    "    \n",
    "    return number_to_question_map[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what'"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'affirmation'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(\"Is there a cab available for airport?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unknown'"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(\"Where do I get good Lebanese food?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what'"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(\"What is the busiest air travel season ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(\"What time does the train leave?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is able to learn the above question belongs to the when category even though it starts with \"what\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
